{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miaortizma/jap2srs/blob/main/V3_JAP_SONG2SRS_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNbq7IkAvwCc"
      },
      "source": [
        "\n",
        "Transcription approach:\n",
        "\n",
        "- Use demucs to split vocal track from drum, bass, and other (yes 'other' is a class on it's own)\n",
        "\n",
        "\n",
        "10-nov-2024:\n",
        "\n",
        "Notes for improvement:\n",
        "\n",
        "- integrate git\n",
        "\n",
        "Add pitch accent to notes:\n",
        "- Update many decks at the same time (scale up)\n",
        "- If we have kana readings, use them to avoid mismatch between reading and OJAD\n",
        "- Remove reading from raw material for rare readings (e.g at hyouri ittai)\n",
        "\n",
        "Lyrics syncing\n",
        "- Only apply furigana to kanji! Not hiragana or katana, nor endings  (i.e put furigana on ki of kita but not on ta)\n",
        "\n",
        "**Lyrics Scraping Approach (31-Oct):**\n",
        "\n",
        "**Quick Summary:**\n",
        "\n",
        "\n",
        "1. **Initial Search:** Take the transcribed lyrics and combine the first few verses into a single string, `S`.\n",
        "2. **Google Search:** Search for `S` on Google to locate the lyrics page.\n",
        "3. **Extract Lyrics:** Use BeautifulSoup to fetch the entire lyrics text.\n",
        "4. **Verse Splitting:** Split verses using `<br>` as the delimiter.\n",
        "5. **Fuzzy Matching:** Apply fuzzy matching (e.g., Levenshtein distance) to align verses with Whisper segments.\n",
        "\n",
        "**Details & Notes:**\n",
        "\n",
        "- `get_text()` typically returns all paragraphs in a single string rather than separate paragraphs. As lyrics are usually displayed together on websites, using `get_text()` tends to merge them into one string. By default, BeautifulSoup replaces `<br>` with spaces, but replacing `<br>` with a unique separator helps maintain verse separation.\n",
        "- After identifying the best match, fuzzy search can be further used for word- or character-level matching within segments in the final implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_PKxVwJjfpV"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "pykakasi\n",
        "genanki\n",
        "openai-whisper\n",
        "whisper-timestamped\n",
        "yt-dlp==2024.10.22\n",
        "fuzzywuzzy\n",
        "fuzzysearch\n",
        "pydub\n",
        "google-colab-selenium\n",
        "selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clni3TwsjTUq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install fonts-noto\n",
        "!pip install uv\n",
        "!uv pip install --system -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcptrOIB7rKP"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "#!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "#!pip install -U bitsandbytes triton trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A3-7z7yHPMm"
      },
      "source": [
        "# 2024.4.9\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_a3pvYSUTeCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSGpCjILCIcG"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# YouTubeから動画をダウンロードする関数\n",
        "def download_youtube_video(url):\n",
        "\n",
        "    youtube_info = get_youtube_info(url)\n",
        "    song_title = youtube_info[\"song_title\"]\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': f'{song_title}.%(ext)s',  # ファイル名のテンプレート\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(url, download=True)\n",
        "        output_filename = ydl.prepare_filename(info_dict)\n",
        "    print(f\"downloaded {output_filename}\")\n",
        "\n",
        "    return output_filename\n",
        "\n",
        "def get_song_title(input_string):\n",
        "    # 正規表現パターン：日本語のタイトルは「」で囲まれている\n",
        "    pattern = r'「(.*?)」'\n",
        "    match = re.search(pattern, input_string)\n",
        "\n",
        "    if match:\n",
        "        title = match.group(1)\n",
        "    else:\n",
        "        title = input_string\n",
        "\n",
        "    title = title.replace(' ', '_')\n",
        "\n",
        "    return title\n",
        "\n",
        "# YouTubeからタイトルと説明を取得する関数\n",
        "def get_youtube_info(url):\n",
        "    ydl_opts = {\n",
        "        'quiet': True,\n",
        "        'format': 'bestaudio/best',\n",
        "        'extract_flat': True,\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=False)\n",
        "        response = {\n",
        "            'title': info.get('title', None),\n",
        "            'description': info.get('description', None)\n",
        "\n",
        "        }\n",
        "        if 'title' in response:\n",
        "            response['song_title'] = get_song_title(response['title'])\n",
        "\n",
        "        return response\n",
        "\n",
        "# Whisperで音声を解析し、タイムスタンプ付きの歌詞を抽出する関数\n",
        "def transcribe_audio(audio_path, initial_prompt=None):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"segments\"]\n",
        "\n",
        "# タイムスタンプ付きの歌詞をCSVファイルに保存する関数\n",
        "def save_to_csv(segments, csv_path):\n",
        "    data = []\n",
        "    for segment in segments:\n",
        "        start = segment[\"start\"]\n",
        "        end = segment[\"end\"]\n",
        "        text = segment[\"text\"]\n",
        "        data.append([start, end, text])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"start\", \"end\", \"text\"])\n",
        "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "## scraping.py\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "import random\n",
        "\n",
        "# プロキシのリスト（例として一部のプロキシを使用）\n",
        "proxies = [\n",
        "    'http://123.123.123.123:8080',\n",
        "    'http://124.124.124.124:8080',\n",
        "    # 他のプロキシを追加\n",
        "]\n",
        "\n",
        "# ユーザーエージェントのリスト\n",
        "user_agents = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',\n",
        "    # 他のユーザーエージェントを追加\n",
        "]\n",
        "\n",
        "# Google検索を使用してリンクを取得する関数\n",
        "def get_lyrics_links(query):\n",
        "    query = f\"{query} site:azlyrics.biz\"\n",
        "    results = search(query, stop=1)\n",
        "    return list(results)\n",
        "\n",
        "def get_soup(url):\n",
        "    proxy = {'http': random.choice(proxies)}\n",
        "    user_agent = {'User-Agent': random.choice(user_agents)}\n",
        "\n",
        "    response = requests.get(url, headers=user_agent, proxies=proxy)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    return soup\n",
        "\n",
        "def parse_lyrics(line):\n",
        "    line = line.replace(\"\\u3000\", \" \")\n",
        "    return line\n",
        "\n",
        "# URLから歌詞をスクレイピングする関数\n",
        "def scrape_lyrics(url):\n",
        "\n",
        "    soup = get_soup(url)\n",
        "    lyrics = []\n",
        "    konten_divs = soup.find_all('div', id='konten')\n",
        "\n",
        "    if len(konten_divs) > 1:\n",
        "        next_element = konten_divs[0].find_next_sibling()\n",
        "        while next_element and next_element != konten_divs[1]:\n",
        "            if next_element.name == 'p':\n",
        "                verses = next_element.get_text().split(\"\\n\")\n",
        "\n",
        "                verses = [parse_lyrics(verse) for verse in verses]\n",
        "                lyrics += verses\n",
        "            next_element = next_element.find_next_sibling()\n",
        "        return lyrics\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "from IPython.display import Video\n",
        "import pykakasi\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "\n",
        "def kks_to_passport(text):\n",
        "    kks = pykakasi.kakasi()\n",
        "    kakasi_result = kks.convert(text)\n",
        "\n",
        "    text = \"\"\n",
        "    for item in kakasi_result:\n",
        "        text += item[\"passport\"]\n",
        "\n",
        "    return text\n",
        "\n",
        "def parse_pykakasi_result(kakasi_result):\n",
        "    \"\"\"\n",
        "    Add whitespace before kanji so that furigana parases correctly on phones\n",
        "    \"\"\"\n",
        "    furigana_text = \"\"\n",
        "    for item in kakasi_result:\n",
        "        if item[\"orig\"] == item[\"hira\"] or item[\"orig\"] == item[\"kana\"]:\n",
        "            furigana_text += item[\"orig\"]\n",
        "        else:\n",
        "            furigana = item[\"hira\"]\n",
        "            furigana_text +=  \" \" + item[\"orig\"] + f\"[{furigana}]\"\n",
        "\n",
        "    furigana_text.strip()\n",
        "\n",
        "    return furigana_text\n",
        "\n",
        "def create_assets(df_transcription, youtube_url):\n",
        "    \"\"\"\n",
        "    GenAnki or anki doesn't like assets that their path is inside subfolders,\n",
        "    The files need to be in the ./ folder\n",
        "    \"\"\"\n",
        "\n",
        "    kks = pykakasi.kakasi()\n",
        "\n",
        "    video_info = get_youtube_info(youtube_url)\n",
        "    song_title = video_info[\"song_title\"]\n",
        "\n",
        "    assets_metadata = []\n",
        "\n",
        "    for i, row in df_transcription.iterrows():\n",
        "        start_time = row[\"start\"]\n",
        "        end_time = row[\"end\"]\n",
        "        text = row[\"text\"]\n",
        "\n",
        "        kakasi_result = kks.convert(text)\n",
        "\n",
        "        furigana_text = parse_pykakasi_result(kakasi_result)\n",
        "\n",
        "        song_title = kks_to_passport(song_title)\n",
        "        #os.makedirs(song_title, exist_ok=True)\n",
        "\n",
        "        segment_audio_path = f\"{song_title}_segment_{i}_audio.mp3\"\n",
        "        segment_video_path = f\"{song_title}_segment_{i}.mp4\"\n",
        "\n",
        "        video = VideoFileClip(RAW_FILE_NAME)\n",
        "        clip = video.subclip(start_time, end_time)\n",
        "\n",
        "        #if save_video:\n",
        "        #    clip.write_videofile(segment_video_path, codec=\"libx264\")\n",
        "        if segment_audio_path not in os.listdir(\"./\"):\n",
        "            clip.audio.write_audiofile(segment_audio_path)\n",
        "\n",
        "        pitch_graph_path = f\"{song_title}_segment_{i}_pitch\" # get_pitched_text adds .png\n",
        "\n",
        "        pitch_graph_path = get_pitched_text(text, filename=pitch_graph_path)\n",
        "\n",
        "        assets_metadata.append({\n",
        "            \"expression\": text,\n",
        "            \"furigana\": furigana_text,\n",
        "            \"audio\": segment_audio_path,\n",
        "            \"pitch_graph\": pitch_graph_path\n",
        "        })\n",
        "\n",
        "    return assets_metadata\n",
        "\n",
        "\n",
        "from time import sleep\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import google_colab_selenium as gs\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "custom_options = Options()\n",
        "# Add your custom options here\n",
        "custom_options.add_argument('--lang=ja-JP')  # 日本語に設定\n",
        "custom_options.add_argument(\"--enable-javascript\")\n",
        "\n",
        "\n",
        "driver = gs.Chrome(options=custom_options)\n",
        "\n",
        "#this function downloads the pitch accent graph,\n",
        "#the pitch accent graphs are saved in the pitch_graph folder as png\n",
        "def get_pitched_text(text,filename=None):\n",
        "# https://github.com/Tarikhoza/anki_add_ojad_pitch_plugin\n",
        "    if \"pitch_graph\" not in os.listdir():\n",
        "        os.mkdir(\"pitch_graph\")\n",
        "\n",
        "    #remove html tags and &nbsp;\n",
        "    text = BeautifulSoup(f\"<div>{text}</div>\", \"html.parser\" ).get_text().replace(\"\\xa0\",\"\")\n",
        "\n",
        "    if filename == None:\n",
        "        filename = text\n",
        "    if f\"{filename}.png\" in os.listdir(\"./\"):\n",
        "        return f\"{filename}.png\"\n",
        "    try:\n",
        "        print(\"Getting graph from OJAD\", text)\n",
        "        driver.get(\"https://www.gavo.t.u-tokyo.ac.jp/ojad/phrasing\")\n",
        "\n",
        "        #putting text into the input field\n",
        "        input_element = driver.find_element(By.ID,\"PhrasingText\")\n",
        "        input_element.send_keys(text);\n",
        "\n",
        "        #pressing the submit button and waiting for 5 seconds\n",
        "        submit_button = driver.find_element(By.ID, \"phrasing_submit_wrapper\").find_element(By.TAG_NAME,\"input\")\n",
        "        submit_button.click()\n",
        "        sleep(5)\n",
        "\n",
        "        #change css to remove unnecessery elements from page before making screenshot\n",
        "        driver.execute_script(\"\"\"\n",
        "            var styleElement = document.createElement('style');\n",
        "            styleElement.innerText = `input{display:none;} font{display:none} select{display:none} *{padding:0; margin:0;} #phrasing_main{width:fit-content} .ds_t{display:none}`\n",
        "            document.body.appendChild(styleElement);\n",
        "            const elements = document.querySelectorAll('*');\n",
        "            elements.forEach(element => {\n",
        "              element.style.cssText = 'background-color:white';\n",
        "            });\n",
        "        \"\"\")\n",
        "\n",
        "        #making a screenshot of the generated pitch accent graph\n",
        "        driver.save_screenshot(\"test.png\")\n",
        "        driver.find_element(By.ID,\"phrasing_main\").screenshot(f\"{filename}.png\")\n",
        "        return f\"{filename}.png\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error: \", e)\n",
        "\n",
        "\n",
        "import hashlib\n",
        "\n",
        "def generate_deck_id(song_title):\n",
        "  \"\"\"Generates a deck ID based on a hash of the song title.\"\"\"\n",
        "  hash_object = hashlib.md5(song_title.encode())\n",
        "  hex_dig = hash_object.hexdigest()\n",
        "  deck_id = int(hex_dig, 16) % (1 << 30)  # Ensure it's within the desired range\n",
        "  return deck_id\n",
        "\n",
        "## anki_utils.py\n",
        "import genanki\n",
        "#import random\n",
        "#print(random.randrange(1 << 30, 1 << 31))\n",
        "\n",
        "ANKI_MODEL_NO = 2133834403\n",
        "\n",
        "class Subs2SRSNote(genanki.Note):\n",
        "  @property\n",
        "  def guid(self):\n",
        "    return genanki.guid_for(self.fields[0], self.fields[1])\n",
        "\n",
        "SONG_MODEL = genanki.Model(\n",
        "  ANKI_MODEL_NO,\n",
        "  'Song2SRS',\n",
        "  fields=[\n",
        "    {'name': 'Expression'},\n",
        "    {'name': 'Furigana'},\n",
        "    {'name': 'Audio'},\n",
        "    {'name': 'Pitch'}\n",
        "  ],\n",
        "  templates=[\n",
        "    {\n",
        "      'name': 'Card',\n",
        "      'qfmt': '{{Expression}}<br>',\n",
        "      'afmt': '{{FrontSide}}<hr id=\"answer\">{{furigana:Furigana}}<br>{{Audio}}<br>{{Pitch}}',\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "def create_anki_deck(song_title, assets):\n",
        "\n",
        "    song_deck = genanki.Deck(\n",
        "        ANKI_MODEL_NO,\n",
        "        song_title\n",
        "    )\n",
        "\n",
        "    media_files = []\n",
        "\n",
        "    for i, asset in enumerate(assets):\n",
        "\n",
        "        audio_path = asset[\"audio\"]\n",
        "        pitch_graph_path = asset[\"pitch_graph\"]\n",
        "\n",
        "        song_note = genanki.Note(\n",
        "            model=SONG_MODEL,\n",
        "            fields=[\n",
        "                asset[\"expression\"],\n",
        "                asset[\"furigana\"],\n",
        "                f\"[sound:{audio_path}]\",\n",
        "                f'<img src=\"{pitch_graph_path}\">'\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        song_deck.add_note(song_note)\n",
        "\n",
        "        media_files.append(audio_path)\n",
        "        media_files.append(pitch_graph_path)\n",
        "\n",
        "    anki_package = genanki.Package(song_deck)\n",
        "    anki_package.media_files = media_files\n",
        "\n",
        "    return anki_package\n",
        "\n",
        "def create_anki_deck_from_assets(assets, youtube_url):\n",
        "\n",
        "    video_info = get_youtube_info(youtube_url)\n",
        "    song_title = video_info[\"song_title\"]\n",
        "\n",
        "    song_deck_package = create_anki_deck(song_title, assets)\n",
        "\n",
        "\n",
        "    package_path = f'{song_title}.apkg'\n",
        "    song_deck_package.write_to_file(package_path)\n",
        "\n",
        "    print(f\"Deck saved to {package_path}\")\n",
        "\n",
        "    return package_path\n",
        "\n",
        "# prompt: using torchaudio, convert a mp4 to a .wav, and load the .wav and embed it in the notebook to validate the conversion\n",
        "\n",
        "import torchaudio\n",
        "import os\n",
        "\n",
        "def convert_mp4_to_wav(mp4_path, wav_path):\n",
        "  \"\"\"Converts an MP4 file to WAV using torchaudio.\"\"\"\n",
        "  waveform, sample_rate = torchaudio.load(mp4_path)\n",
        "  torchaudio.save(wav_path, waveform, sample_rate)\n",
        "\n",
        "\n",
        "def load_and_embed_wav(wav_path):\n",
        "  \"\"\"Loads a WAV file and embeds it in the notebook for validation.\"\"\"\n",
        "  waveform, sample_rate = torchaudio.load(wav_path)\n",
        "  print(f\"Loaded WAV file: {wav_path}\")\n",
        "  print(f\"Sample rate: {sample_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tRYeF9eYcUpx"
      },
      "outputs": [],
      "source": [
        "# @title Download and transcribe video\n",
        "youtube_url = \"https://www.youtube.com/watch?v=eKoD2CRr_KA&ab_channel=yuzuofficial\" # @param {type:\"string\"}\n",
        "\n",
        "# 実行部分\n",
        "video_path = 'video.mp4'\n",
        "csv_path = 'lyrics.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku9MZGgVHxOS"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "RAW_FILE_NAME = download_youtube_video(youtube_url)\n",
        "\n",
        "video_info = get_youtube_info(youtube_url)\n",
        "song_title = video_info[\"song_title\"]\n",
        "\n",
        "print(f\"{song_title=} {RAW_FILE_NAME=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCba7u-OSuZW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
        "from torchaudio.utils import download_asset\n",
        "import torch\n",
        "import pydub\n",
        "import whisper\n",
        "import whisper_timestamped\n",
        "from torchaudio.transforms import Fade\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "CONVERTED_FILE_NAME = RAW_FILE_NAME.split('.')[0] + '.wav'\n",
        "VOCALS_FILE_NAME = \"vocals.wav\"\n",
        "\n",
        "SPLITS_DIR_NAME = \"splits\"\n",
        "SPLITS_TIMESTAMPS_FILE_NAME = \"timestamps.txt\"\n",
        "TRANSCRIPTION_FILE_NAME = \"transcription.json\"\n",
        "\n",
        "SPLITS_PADDING = 2000  # ms\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def separate_sources(\n",
        "    model,\n",
        "    mix,\n",
        "    segment=10.0,\n",
        "    overlap=0.1,\n",
        "    device=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
        "\n",
        "    Args:\n",
        "        segment (int): segment length in seconds\n",
        "        device (torch.device, str, or None): if provided, device on which to\n",
        "            execute the computation, otherwise `mix.device` is assumed.\n",
        "            When `device` is different from `mix.device`, only local computations will\n",
        "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = mix.device\n",
        "    else:\n",
        "        device = torch.device(device)\n",
        "\n",
        "    batch, channels, length = mix.shape\n",
        "\n",
        "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
        "    start = 0\n",
        "    end = chunk_len\n",
        "    overlap_frames = overlap * sample_rate\n",
        "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
        "\n",
        "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
        "\n",
        "    while start < length - overlap_frames:\n",
        "        logger.debug(f\"Demucs source separation: {start=}\")\n",
        "        chunk = mix[:, :, start:end]\n",
        "        with torch.no_grad():\n",
        "            out = model.forward(chunk)\n",
        "        out = fade(out)\n",
        "        final[:, :, :, start:end] += out\n",
        "        if start == 0:\n",
        "            fade.fade_in_len = int(overlap_frames)\n",
        "            start += int(chunk_len - overlap_frames)\n",
        "        else:\n",
        "            start += chunk_len\n",
        "        end += chunk_len\n",
        "        if end >= length:\n",
        "            fade.fade_out_len = 0\n",
        "    return final\n",
        "\n",
        "def extract_voice(target_location: str = './') -> str:\n",
        "    \"\"\"\n",
        "    https://pytorch.org/audio/main/tutorials/hybrid_demucs_tutorial.html#spectrograms-and-audio\n",
        "    \"\"\"\n",
        "    logger.info(\"Extracting vocals\")\n",
        "    bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "    model = bundle.get_model()\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # We download the audio file from our storage. Feel free to download another file and use audio from a specific path\n",
        "    song_file = os.path.join(target_location, CONVERTED_FILE_NAME)\n",
        "    waveform, sample_rate = torchaudio.load(\n",
        "        song_file\n",
        "    )  # replace SAMPLE_SONG with desired path for different song\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    if sample_rate != 44100:\n",
        "        logger.warn(\"Warn: Resampling to 44100Hz\", sample_rate=sample_rate)\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, 44100)\n",
        "        sample_rate = 44100\n",
        "\n",
        "    # parameters\n",
        "    segment: int = 10\n",
        "    overlap = 0.1\n",
        "\n",
        "    ref = waveform.mean(0)\n",
        "    waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
        "\n",
        "    sources = separate_sources(\n",
        "        model,\n",
        "        waveform[None],\n",
        "        device=device,\n",
        "        segment=segment,\n",
        "        overlap=overlap,\n",
        "    )[0]\n",
        "    sources = sources * ref.std() + ref.mean()\n",
        "\n",
        "    sources_list = model.sources\n",
        "    sources = list(sources)\n",
        "\n",
        "    audios = dict(zip(sources_list, sources))\n",
        "\n",
        "    output_file_name = os.path.join(target_location, VOCALS_FILE_NAME)\n",
        "    torchaudio.save(output_file_name, audios[\"vocals\"], sample_rate)\n",
        "\n",
        "    return output_file_name\n",
        "\n",
        "def split(target_location: str = './') -> str:\n",
        "    logger.info(\"Splitting vocals\")\n",
        "    vocals_file = os.path.join(target_location, VOCALS_FILE_NAME)\n",
        "    sound = pydub.AudioSegment.from_file(vocals_file, format=\"wav\")\n",
        "    chunk_timestamps = pydub.silence.detect_nonsilent(\n",
        "        sound, min_silence_len=5000, silence_thresh=-32\n",
        "    )\n",
        "\n",
        "    chunk_timestamps = [\n",
        "        (\n",
        "            max(chunk_timestamps[i][0] - SPLITS_PADDING, 0),\n",
        "            min(chunk_timestamps[i][1] + SPLITS_PADDING, len(sound)),\n",
        "        )\n",
        "        for i in range(len(chunk_timestamps))\n",
        "    ]\n",
        "    chunks = [\n",
        "        sound[chunk_timestamps[i][0] : chunk_timestamps[i][1]]\n",
        "        for i in range(len(chunk_timestamps))\n",
        "    ]\n",
        "\n",
        "    splits_dir = os.path.join(target_location, SPLITS_DIR_NAME)\n",
        "    if not os.path.exists(splits_dir):\n",
        "        os.makedirs(splits_dir)\n",
        "\n",
        "    logger.info(\"Splitting vocals\", chunks_count=len(chunks))\n",
        "    for i in range(len(chunks)):\n",
        "        chunk = chunks[i]\n",
        "\n",
        "        chunk.export(\n",
        "            os.path.join(splits_dir, f\"{i}.wav\"),\n",
        "            format=\"wav\",\n",
        "        )\n",
        "\n",
        "    with open(os.path.join(splits_dir, SPLITS_TIMESTAMPS_FILE_NAME), \"w\") as f:\n",
        "        for timestamp in chunk_timestamps:\n",
        "            f.write(f\"{timestamp[0]} {timestamp[1]}\\n\")\n",
        "\n",
        "    return os.path.join(target_location, SPLITS_DIR_NAME)\n",
        "\n",
        "\n",
        "def transcribe_segments(target_location: str = './', language: str = 'japanese'):\n",
        "    logger.info(\"Transcribing audio\")\n",
        "    splits_dir = os.path.join(target_location, SPLITS_DIR_NAME)\n",
        "    model = whisper_timestamped.load_model(\"openai/whisper-small\", device=\"cuda\")\n",
        "    logger.debug(\"Model loaded\")\n",
        "\n",
        "    # read timestamp delays\n",
        "    with open(os.path.join(splits_dir, SPLITS_TIMESTAMPS_FILE_NAME), \"r\") as f:\n",
        "        chunk_timestamps = [\n",
        "            [float(i) for i in line.split(\" \")]\n",
        "            for line in f.readlines()\n",
        "            if line.strip()\n",
        "        ]\n",
        "    logger.debug(\"Timestamps loaded\", chunk_timestamps=chunk_timestamps)\n",
        "\n",
        "    full_transcription = {\"segments\": [], \"text\": \"\"}\n",
        "\n",
        "    whisper_model = whisper.load_model(\"small\")\n",
        "\n",
        "    transcription_total = []\n",
        "\n",
        "    # list files in splits_dir\n",
        "    for i in tqdm(range(len(chunk_timestamps)), desc=\"Transcribing\"):\n",
        "        logger.info(f\"transcribing split {i}\")\n",
        "        file_name = f\"{i}.wav\"\n",
        "\n",
        "        transcription = whisper_model.transcribe(\n",
        "            audio=os.path.join(splits_dir, file_name),\n",
        "            temperature=0,\n",
        "            condition_on_previous_text=False,\n",
        "            verbose=True,\n",
        "            language=language\n",
        "        )\n",
        "\n",
        "        timestamp_adjustment = chunk_timestamps[i][0] / 1000\n",
        "\n",
        "        data = []\n",
        "        for segment in transcription[\"segments\"]:\n",
        "            start = segment[\"start\"] + timestamp_adjustment\n",
        "            end = segment[\"end\"] + timestamp_adjustment\n",
        "            text = segment[\"text\"]\n",
        "            data.append([start, end, text])\n",
        "\n",
        "        transcription_total += data\n",
        "\n",
        "\n",
        "    csv_path = 'transcription.csv'\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(transcription_total, columns=[\"start\", \"end\", \"text\"])\n",
        "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "\n",
        "\n",
        "def transcribe(target_location: str = './', language: str = 'japanese'):\n",
        "    logger.info(\"Transcribing audio\")\n",
        "    splits_dir = os.path.join(target_location, SPLITS_DIR_NAME)\n",
        "    model = whisper_timestamped.load_model(\"openai/whisper-small\", device=\"cuda\")\n",
        "    logger.debug(\"Model loaded\")\n",
        "\n",
        "    # read timestamp delays\n",
        "    with open(os.path.join(splits_dir, SPLITS_TIMESTAMPS_FILE_NAME), \"r\") as f:\n",
        "        chunk_timestamps = [\n",
        "            [float(i) for i in line.split(\" \")]\n",
        "            for line in f.readlines()\n",
        "            if line.strip()\n",
        "        ]\n",
        "    logger.debug(\"Timestamps loaded\", chunk_timestamps=chunk_timestamps)\n",
        "\n",
        "    full_transcription = {\"segments\": [], \"text\": \"\"}\n",
        "\n",
        "    # list files in splits_dir\n",
        "    for i in tqdm(range(len(chunk_timestamps)), desc=\"Transcribing\"):\n",
        "        file_name = f\"{i}.wav\"\n",
        "\n",
        "        result = whisper_timestamped.transcribe(\n",
        "            model,\n",
        "            temperature=0,\n",
        "            audio=os.path.join(splits_dir, file_name),\n",
        "            task=\"transcribe\",\n",
        "            condition_on_previous_text=False,\n",
        "            language=language,\n",
        "        )\n",
        "        timestamp_adjustment = chunk_timestamps[i][0] / 1000\n",
        "\n",
        "        result_adjusted = {\n",
        "            \"text\": result[\"text\"],\n",
        "            \"segments\": [\n",
        "                {\n",
        "                    **s,\n",
        "                    \"end\": s[\"end\"] + timestamp_adjustment,\n",
        "                    \"start\": s[\"start\"] + timestamp_adjustment,\n",
        "                    \"words\": [\n",
        "                        {\n",
        "                            **w,\n",
        "                            \"end\": w[\"end\"] + timestamp_adjustment,\n",
        "                            \"start\": w[\"start\"] + timestamp_adjustment,\n",
        "                        }\n",
        "                        for w in s[\"words\"]\n",
        "                    ],\n",
        "                }\n",
        "                for s in result[\"segments\"]\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        full_transcription = {\n",
        "            \"text\": full_transcription[\"text\"] + result_adjusted[\"text\"],\n",
        "            \"segments\": full_transcription[\"segments\"] + result_adjusted[\"segments\"],\n",
        "        }\n",
        "\n",
        "    with open(os.path.join(target_location, TRANSCRIPTION_FILE_NAME), \"w\") as f:\n",
        "        json.dump(full_transcription, f)\n",
        "\n",
        "\"\"\"\n",
        "DONT DELETE\n",
        "extract_voice()\n",
        "transcribe_segments()\n",
        "pd.read_csv(\"transcription.csv\")\n",
        "transcribe(language=\"japanese\")\n",
        "with open(\n",
        "    os.path.join('./', TRANSCRIPTION_FILE_NAME), \"r\"\n",
        ") as f:\n",
        "    data = json.load(f)\n",
        "segments = [d[\"text\"] for d in data[\"segments\"]]\n",
        "\n",
        "segments\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpk8GQWIV2GD"
      },
      "outputs": [],
      "source": [
        "convert_mp4_to_wav(RAW_FILE_NAME, CONVERTED_FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISn5woOd4JXy"
      },
      "source": [
        "# Apply whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncLXFofXtllj"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import AudioFileClip\n",
        "from IPython.display import Audio\n",
        "import torchaudio\n",
        "\n",
        "def show_audio_segment(audio_file_path, start_time, end_time):\n",
        "    \"\"\"\n",
        "    Loads an audio file, extracts a segment, and plays it.\n",
        "\n",
        "    Args:\n",
        "        audio_file_path: Path to the audio file.\n",
        "        start_time: Start time of the segment in seconds.\n",
        "        end_time: End time of the segment in seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    video = AudioFileClip(audio_file_path)\n",
        "    clip = video.subclip(start_time, end_time)\n",
        "    clip.write_audiofile(\"test.wav\")\n",
        "    waveform, sample_rate = torchaudio.load(\"test.wav\")\n",
        "    return Audio(waveform, rate=sample_rate)\n",
        "\n",
        "from moviepy.editor import AudioFileClip\n",
        "from IPython.display import Audio\n",
        "\n",
        "show_audio_segment(CONVERTED_FILE_NAME, 5, 13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4dIgwTky4bE"
      },
      "outputs": [],
      "source": [
        "\"\"\"import torch, gc\n",
        "\n",
        "del whisper_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "px63114V0zPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4ayCzQPg0liQ"
      },
      "outputs": [],
      "source": [
        "model = whisper_timestamped.load_model(\"openai/whisper-medium\", device=device)\n",
        "\n",
        "transcription = whisper_timestamped.transcribe(\n",
        "    model,\n",
        "    temperature=0,\n",
        "    audio=CONVERTED_FILE_NAME,\n",
        "    task=\"transcribe\",\n",
        "    condition_on_previous_text=False,\n",
        "    language=\"japanese\",\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRqxTJKQBiB1"
      },
      "outputs": [],
      "source": [
        "\"\"\"import whisper\n",
        "\n",
        "whisper_model = whisper.load_model(\"small\", device=\"cuda\")\n",
        "\n",
        "transcription = whisper_model.transcribe(\n",
        "    audio=CONVERTED_FILE_NAME,\n",
        "    temperature=0,\n",
        "    condition_on_previous_text=False,\n",
        "    verbose=True,\n",
        "    language=\"japanese\"\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RLMciXlqUfX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Word(BaseModel):\n",
        "    text: str\n",
        "    start: float\n",
        "    end: float\n",
        "    confidence: float\n",
        "\n",
        "\n",
        "class TranscriptionSegment(BaseModel):\n",
        "    id: int\n",
        "    seek: int\n",
        "    start: float\n",
        "    end: float\n",
        "    text: str\n",
        "    tokens: List[int]\n",
        "    temperature: float\n",
        "    avg_logprob: float\n",
        "    compression_ratio: float\n",
        "    no_speech_prob: float\n",
        "    confidence: float\n",
        "    words: List[Word]\n",
        "\n",
        "import unicodedata\n",
        "import pykakasi\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "\n",
        "def fuzzy_match_text(all_text, text_to_match):\n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "\n",
        "    for line in all_text:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            score = fuzz.ratio(line, text_to_match)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_match = line\n",
        "\n",
        "    return best_match, best_score\n",
        "\n",
        "def translate_to_kana(text):\n",
        "    kks = pykakasi.kakasi()\n",
        "    kks_result = kks.convert(text)\n",
        "    result = \"\"\n",
        "    for _ in kks_result:\n",
        "        # Break english sentences into different words\n",
        "        if _[\"kana\"] == _[\"orig\"]:\n",
        "            result += _[\"orig\"]\n",
        "        elif _[\"hira\"] == _[\"orig\"]:\n",
        "            result += _[\"hira\"]\n",
        "        else:\n",
        "            result += _[\"hira\"]\n",
        "    return result\n",
        "\n",
        "\n",
        "def translate_to_romaji(text):\n",
        "    return ' '.join(translate_to_romaji_list(text))\n",
        "\n",
        "def translate_to_romaji_list(text):\n",
        "    kks = pykakasi.kakasi()\n",
        "    result = kks.convert(text)\n",
        "    romaji = []\n",
        "    for _ in result:\n",
        "        # Break english sentences into different words\n",
        "        if _[\"hepburn\"] == _[\"orig\"]:\n",
        "            romaji += _[\"orig\"].split(\" \")\n",
        "        else:\n",
        "            romaji += [_[\"hepburn\"]]\n",
        "    return romaji\n",
        "\n",
        "def get_kakasi_results(text):\n",
        "    kks = pykakasi.kakasi()\n",
        "    result = kks.convert(text)\n",
        "    romaji = []\n",
        "    original = []\n",
        "    for _ in result:\n",
        "        # Break english sentences into different words\n",
        "        if _[\"hepburn\"] == _[\"orig\"]:\n",
        "            original += _[\"orig\"].split(\" \")\n",
        "            romaji += _[\"orig\"].split(\" \")\n",
        "        else:\n",
        "            original += [_[\"orig\"]]\n",
        "            romaji += [_[\"hepburn\"]]\n",
        "\n",
        "\n",
        "    return list(zip(original, romaji))\n",
        "\n",
        "# iterative approach:\n",
        "# - after finding the best match\n",
        "# - add the previous segment of transcriptions test, and the next one as well, if they exists (handle edge cases)\n",
        "# - also, apply pikakasi to turn it into romaji, to ease the search, try adding 1 to 5 tokenes from both the previous and the next segment\n",
        "# - this is combinatorial so we'd have 6 * 6 (considering the 0 add in either direction) possible variations to get the best fuzzy score from\n",
        "\n",
        "def beam_search(verse, transcription_texts, current_index):\n",
        "\n",
        "    current_match = transcription_texts[current_index]\n",
        "    best_match_romaji = translate_to_romaji(current_match)\n",
        "    # Initialize tracking for the best combination\n",
        "\n",
        "    verse_romaji = translate_to_romaji(verse)\n",
        "    best_combination = best_match_romaji\n",
        "    best_prev_tokens = 0\n",
        "    best_next_tokens = 0\n",
        "    best_score = fuzz.ratio(best_match_romaji, verse_romaji)\n",
        "\n",
        "    combinations = [best_match_romaji]\n",
        "\n",
        "    previous_segment = translate_to_romaji_list(transcription_texts[current_index - 1]) if current_index > 0 else []\n",
        "    next_segment = translate_to_romaji_list(transcription_texts[current_index + 1]) if current_index < len(transcription_texts) - 1 else []\n",
        "\n",
        "    max_prev_tokens = min(len(previous_segment), 10)\n",
        "    max_next_tokens = min(len(next_segment), 10)\n",
        "\n",
        "    for i in range(max_prev_tokens + 1):  # Up to max_prev_tokens from previous segment\n",
        "        for j in range(max_next_tokens + 1):  # Up to max_next_tokens from next segment\n",
        "            # Start with the original match\n",
        "            combination = best_match_romaji\n",
        "\n",
        "            # Add tokens from the previous segment if available\n",
        "            if len(previous_segment) > 0 and i > 0:\n",
        "                combination = ' '.join(previous_segment[-i:]) + ' ' + combination\n",
        "\n",
        "            # Add tokens from the next segment if available\n",
        "            if len(next_segment) > 0 and j > 0:\n",
        "                combination = combination + ' ' + ' '.join(next_segment[:j])\n",
        "\n",
        "            # Calculate the fuzzy score for this combination\n",
        "            score = fuzz.ratio(combination, verse_romaji)\n",
        "\n",
        "            # Update if this combination is the best so far\n",
        "            if score > best_score:\n",
        "                best_combination = combination\n",
        "                best_score = score\n",
        "                best_prev_tokens = i\n",
        "                best_next_tokens = j\n",
        "\n",
        "    return best_combination, current_index, best_prev_tokens, best_next_tokens, best_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBjr4JaXqb6a"
      },
      "outputs": [],
      "source": [
        "# todo: save as json\n",
        "transcription_segments = [TranscriptionSegment(**segment) for segment in transcription[\"segments\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RD5MKIil1C9h"
      },
      "outputs": [],
      "source": [
        "transcription_segments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"test.csv\""
      ],
      "metadata": {
        "id": "y8-Q_gAKZEUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFzr6DbVqQsR"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for segment in transcription_segments:\n",
        "    start = segment.start\n",
        "    end = segment.end\n",
        "    text = segment.text\n",
        "    data.append([start, end, text])\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"start\", \"end\", \"text\"])\n",
        "df.to_csv(csv_path, index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrRfA5RkN_D0"
      },
      "outputs": [],
      "source": [
        "df_transcription = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transcription"
      ],
      "metadata": {
        "id": "SiNgjBMTbARD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pseKZ_fvBp0i"
      },
      "outputs": [],
      "source": [
        "query = ' '.join(df_transcription.iloc[2:5][\"text\"].tolist())\n",
        "\n",
        "result = list(search(f\"{query} site:.jp\", stop=3))\n",
        "result\n",
        "\n",
        "print(result)\n",
        "\n",
        "proxy = {'http': random.choice(proxies)}\n",
        "user_agent = {'User-Agent': random.choice(user_agents)}\n",
        "\n",
        "url = result[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "driver = gs.Chrome(options=custom_options)\n",
        "\n",
        "driver.get(url)\n",
        "html_content = driver.page_source\n"
      ],
      "metadata": {
        "id": "MCXwGPRDaWDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlIPbKFDFord"
      },
      "outputs": [],
      "source": [
        "#response = requests.get(url, headers=user_agent, proxies=proxy)\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# https://stackoverflow.com/questions/61421079/beautifulsoup-get-text-ignoring-line-breaks-br/61423104\n",
        "delimiter = '###'                           # unambiguous string\n",
        "for line_break in soup.findAll('br'):       # loop through line break tags\n",
        "    line_break.replaceWith(delimiter)\n",
        "\n",
        "all_soup_text = soup.get_text('###').splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_soup_text"
      ],
      "metadata": {
        "id": "25LXMdJxaE_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H_QWyEoLetN"
      },
      "outputs": [],
      "source": [
        "all_lyrics = ' '.join(df_transcription[\"text\"].tolist())\n",
        "best_match_whole_lyrics, best_score = fuzzy_match_text(all_soup_text, all_lyrics)\n",
        "\n",
        "best_match_whole_lyrics = unicodedata.normalize('NFKC', best_match_whole_lyrics)\n",
        "verses = best_match_whole_lyrics.split(\"###\")\n",
        "\n",
        "verses = [verse for verse in verses if len(verse) > 0]\n",
        "\n",
        "seen = set()\n",
        "verses = [verse for verse in verses if verse not in seen and not seen.add(verse)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verses"
      ],
      "metadata": {
        "id": "YmaAmB53b3xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kH34pqz_-HG"
      },
      "outputs": [],
      "source": [
        "transcription_texts = df_transcription[\"text\"].tolist()\n",
        "\n",
        "romaji_transcriptions = [translate_to_romaji(_) for _ in transcription_texts]\n",
        "\n",
        "best_combinations = []\n",
        "\n",
        "for i_verse, verse in enumerate(verses):\n",
        "\n",
        "    best_score = 0\n",
        "\n",
        "    verse_romaji = translate_to_romaji(verse)\n",
        "\n",
        "    scores = [(i, fuzz.ratio(_, verse_romaji)) for i, _ in enumerate(romaji_transcriptions)]\n",
        "\n",
        "    scores = sorted(scores, key=lambda x : x[1], reverse=True)\n",
        "\n",
        "     # Track best match across all beam search results\n",
        "    final_index = -1\n",
        "    final_best_comination = \"\"\n",
        "    final_prev_tokens = 0\n",
        "    final_next_tokens = 0\n",
        "    final_best_score = 0\n",
        "\n",
        "    # Apply beam search to top candidates\n",
        "    for index, initial_score in scores[:5]:\n",
        "        combination, _, prev_tokens, next_tokens, score = beam_search(verse, transcription_texts, index)\n",
        "\n",
        "        if score >= final_best_score:\n",
        "            final_index = index\n",
        "            final_best_comination = combination\n",
        "            final_prev_tokens = prev_tokens\n",
        "            final_next_tokens = next_tokens\n",
        "            final_best_score = score\n",
        "\n",
        "    print(f\"Verse: {verse} {i_verse}\")\n",
        "    print(f\"Transcription Index: {final_index}\")\n",
        "    print(f\"Verse romaji: {verse_romaji}\")\n",
        "    print(f\"Best combination: '{final_best_comination}'\")\n",
        "    print(f\"Score: {best_score}\")\n",
        "    print(f\"Tokens from previous: {final_prev_tokens}, Tokens from next: {final_next_tokens}\")\n",
        "    print(f\"Final Best Score: {final_best_score}\\n\")\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    best_combinations.append((verse, final_index, final_prev_tokens, final_next_tokens))\n",
        "\n",
        "    # Output the best combination with scores and token counts\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLcUeKBl-Fw5"
      },
      "outputs": [],
      "source": [
        "def compile_best_combination(\n",
        "    transcription_segments: List[TranscriptionSegment],\n",
        "    verse: str,\n",
        "    index: int,\n",
        "    prev_tokens: int,\n",
        "    next_tokens: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compiles the best transcription segment combination for a given verse by\n",
        "    analyzing adjacent transcription segments and their token information.\n",
        "\n",
        "    Args:\n",
        "        transcription_segments (List[TranscriptionSegment]): List of transcription segments.\n",
        "        verse (str): The target verse to match.\n",
        "        index (int): Current segment index.\n",
        "        prev_tokens (int): Number of tokens to consider from the previous segment.\n",
        "        next_tokens (int): Number of tokens to consider from the next segment.\n",
        "\n",
        "    Returns:\n",
        "        None: This function modifies the `transcription_segments` in place.\n",
        "    \"\"\"\n",
        "\n",
        "    segment = transcription_segments[index]\n",
        "    transcription_text = segment.text\n",
        "    transcription_start = segment.start\n",
        "    transcription_end = segment.end\n",
        "\n",
        "    words_prepended = \"\"\n",
        "    words_added = \"\"\n",
        "\n",
        "    # Handling the previous segment case if prev_tokens is specified\n",
        "    if prev_tokens and index > 0:\n",
        "        previous_segment_romaji = translate_to_romaji_list(transcription_segments[index - 1].text)\n",
        "\n",
        "        kks_previous = get_kakasi_results(transcription_segments[index - 1].text)\n",
        "\n",
        "        words_to_prepend = ''.join(word[0] for word in kks_previous[-prev_tokens:])\n",
        "        print(\"Words to prepend from previous segment:\", words_to_prepend)\n",
        "\n",
        "        bef_segment = transcription_segments[index - 1]\n",
        "        words_prepended = \"\"\n",
        "\n",
        "        for word in reversed(bef_segment.words):\n",
        "            words_prepended = word.text + words_prepended\n",
        "            transcription_start = word.start\n",
        "            if words_to_prepend in words_prepended:\n",
        "                break\n",
        "\n",
        "    # Handling the next segment case if next_tokens is specified\n",
        "    if next_tokens and index < len(transcription_segments) - 1:\n",
        "        next_segment_romaji = translate_to_romaji_list(transcription_segments[index + 1].text)\n",
        "\n",
        "        kks_transcription = get_kakasi_results(transcription_segments[index + 1].text)\n",
        "        originals = [word[0] for word in kks_transcription]\n",
        "\n",
        "        next_segment = transcription_segments[index + 1]\n",
        "        words_to_add = ''.join(originals[:next_tokens])\n",
        "\n",
        "\n",
        "        for word in next_segment.words:\n",
        "            words_added += word.text\n",
        "            transcription_end = word.end\n",
        "            if words_to_add in words_added:\n",
        "                break\n",
        "\n",
        "\n",
        "    transcription_start -= 0.25\n",
        "    transcription_end += 0.25\n",
        "\n",
        "    transcription_text = words_prepended + transcription_text + words_added\n",
        "\n",
        "    print(f\"{verse=}\")\n",
        "\n",
        "    print(f\"{translate_to_romaji(verse)=}\")\n",
        "\n",
        "    print(f\"{translate_to_romaji(transcription_text)=} {transcription_start=} {transcription_end=}\")\n",
        "\n",
        "    return (verse, transcription_text, transcription_start, transcription_end)\n",
        "\n",
        "\n",
        "#verse, transcription_text, transcription_start, transcription_end = compile_best_combination(transcription_segments, *best_combinations[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IOY2FdYsFn2U"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transcription(BaseModel):\n",
        "    text: str\n",
        "    start: float\n",
        "    end: float\n",
        "\n",
        "final_transcription = []\n",
        "errors = []\n",
        "\n",
        "for _ in best_combinations:\n",
        "    verse, transcription_text, transcription_start, transcription_end = compile_best_combination(transcription_segments, *_)\n",
        "\n",
        "    _data = {\n",
        "        \"text\": verse,\n",
        "        \"whisper_text\": transcription_text,\n",
        "        \"start\": transcription_start,\n",
        "        \"end\": transcription_end,\n",
        "        \"fuzz_ratio\": fuzz.ratio(translate_to_romaji(verse), translate_to_romaji(transcription_text))\n",
        "    }\n",
        "\n",
        "    if _data[\"fuzz_ratio\"] < 60:\n",
        "        errors.append(_data)\n",
        "    else:\n",
        "        final_transcription.append(_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transcriptions_for_cards = pd.DataFrame(final_transcription).groupby(\"text\").first().reset_index().sort_values(by=\"start\").reset_index(drop=True)\n",
        "\n",
        "transcriptions_for_cards"
      ],
      "metadata": {
        "id": "5eIqPL8wLNFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xegNb_mzHK5q"
      },
      "outputs": [],
      "source": [
        "errors_df = pd.DataFrame(errors).groupby(\"text\").first().reset_index().sort_values(by=\"start\").reset_index(drop=True)\n",
        "errors_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHQFt_CJ_kHp"
      },
      "outputs": [],
      "source": [
        "for i, transcription in transcriptions_for_cards.iterrows():\n",
        "\n",
        "    verse = transcription[\"text\"]\n",
        "\n",
        "    print(verse)\n",
        "    print(translate_to_kana(verse))\n",
        "    print(translate_to_romaji(verse))\n",
        "\n",
        "    display(show_audio_segment(CONVERTED_FILE_NAME, transcription[\"start\"], transcription[\"end\"]))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = transcriptions_for_cards.iloc[0]\n",
        "test"
      ],
      "metadata": {
        "id": "jctkjAM7NGSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "import PIL as pil\n",
        "\n",
        "def get_keyframe(video_path, start_time, end_time):\n",
        "    video = VideoFileClip(video_path).subclip(start_time, end_time)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keyframes = []\n",
        "\n",
        "    for t in np.linspace(0, video.duration, num=10):  # Sample 10 frames across the segment\n",
        "        frame = video.get_frame(t)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        kp = sift.detect(gray, None)\n",
        "        keyframes.append((t, len(kp), frame))\n",
        "\n",
        "    # Choose the frame with the most features (highest keypoint count)\n",
        "    keyframe = max(keyframes, key=lambda x: x[1])[2]\n",
        "    return keyframe\n",
        "\n",
        "def extact_segment_thumbnails(df_transcription, youtube_url):\n",
        "\n",
        "    video_info = get_youtube_info(youtube_url)\n",
        "\n",
        "    song_title = video_info[\"song_title\"]\n",
        "\n",
        "    for i, row in df_transcription.iterrows():\n",
        "        start_time = row[\"start\"]\n",
        "        end_time = row[\"end\"]\n",
        "        text = row[\"text\"]\n",
        "        # extract an interesting thumbnail\n",
        "\n",
        "\n",
        "        keyframe = get_keyframe(RAW_FILE_NAME, start_time, end_time)\n",
        "\n",
        "        thumbnail_filename = f\"{song_title}_segment_{i}_thumbnail.png\"\n",
        "\n",
        "        # Convert keyframe to an image and save it\n",
        "        thumbnail_image = pil.Image.fromarray(cv2.cvtColor(keyframe, cv2.COLOR_BGR2RGB))\n",
        "        thumbnail_image.save(thumbnail_filename)\n",
        "\n",
        "        return thumbnail_filename\n",
        "\n",
        "\n",
        "        break\n",
        "\n",
        "thumbnail = extact_segment_thumbnails(transcriptions_for_cards, youtube_url)"
      ],
      "metadata": {
        "id": "w1_bpbUxS-uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "display(Image(filename=thumbnail))\n"
      ],
      "metadata": {
        "id": "5uezT49HVlTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuIxn2QlXKFf"
      },
      "outputs": [],
      "source": [
        "assets = create_assets(transcriptions_for_cards, youtube_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assets"
      ],
      "metadata": {
        "id": "0oStuh4xfPoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss_Jg6v6eivV"
      },
      "outputs": [],
      "source": [
        "package_path = create_anki_deck_from_assets(assets, youtube_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSbrjnAPfOxi"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(package_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise ValueError(\"stop here\")"
      ],
      "metadata": {
        "id": "vMrgN3ZjWPsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOhCIVxDb94i"
      },
      "outputs": [],
      "source": [
        "# Great Days_segment_33_audio.mp3\n",
        "\n",
        "segment = 'Great Days_segment_33_audio.mp3'\n",
        "\n",
        "\n",
        "\n",
        "# prompt: load video.wav into an spectograph, identify the f_0 pitch accent of japanese audio\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as pl\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torchaudio\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load WAV and plot fundamental frequency on spectrogram\n",
        "def plot_f0_on_spectrogram(wav_path):\n",
        "    y, sr = librosa.load(wav_path)\n",
        "\n",
        "    # Compute the spectrogram\n",
        "    D = librosa.stft(y)\n",
        "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "    # Plot spectrogram\n",
        "    plt.figure(figsize=(14, 5))\n",
        "#    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log')\n",
        "  #  plt.colorbar(format='%+2.0f dB')\n",
        " #   plt.title('Spectrogram with Fundamental Frequency (f₀)')\n",
        "\n",
        "    # Compute f₀ using librosa's pyin\n",
        "    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "    # Overlay f₀ on spectrogram\n",
        "    times = librosa.times_like(f0)\n",
        "    plt.plot(times, f0, color='black', label=\"Fundamental Frequency (f₀)\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.show()\n",
        "\n",
        "# File paths\n",
        "segment = 'Great Days_segment_33_audio.mp3'\n",
        "wav_path = segment.replace('.mp3', '.wav')\n",
        "\n",
        "# Convert and plot\n",
        "convert_mp4_to_wav(segment, wav_path)\n",
        "#plot_f0_on_spectrogram(wav_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYT2SvZ0VTsd"
      },
      "source": [
        "# Whisper base model experimentation\n",
        "\n",
        "- Temperature 0 had a strange behaviour: after 8 sentences, it repeats the same transcriptions 149 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hXfvs1WW4uS"
      },
      "source": [
        "# Experiment with pykakasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iswt4ROZifEz"
      },
      "outputs": [],
      "source": [
        "import pykakasi\n",
        "\n",
        "text = \"雲のひれ間に射す 光がほら降り注ぎ\"\n",
        "\n",
        "\n",
        "kks = pykakasi.kakasi()\n",
        "\n",
        "kakasi_result = kks.convert(text)\n",
        "\n",
        "furigana_text = \"\"\n",
        "for item in kakasi_result:\n",
        "    if item[\"orig\"] == item[\"hira\"] or item[\"orig\"] == item[\"kana\"]:\n",
        "        furigana_text += item[\"orig\"]\n",
        "    else:\n",
        "        furigana = item[\"hira\"]\n",
        "        furigana_text +=  \" \" + item[\"orig\"] + f\"[{furigana}]\"\n",
        "\n",
        "furigana_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQGzsD4UmQ5t"
      },
      "outputs": [],
      "source": [
        "kakasi_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSKjwIa-z6F8"
      },
      "source": [
        "# Demucs mix separation\n",
        "\n",
        "\n",
        "https://pytorch.org/audio/main/tutorials/hybrid_demucs_tutorial.html#run-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9bWPbKxz_xE"
      },
      "outputs": [],
      "source": [
        "pip install mir_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1dZRCG2z7d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLfok12Zz-4T"
      },
      "outputs": [],
      "source": [
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "\n",
        "model = bundle.get_model()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "sample_rate = bundle.sample_rate\n",
        "\n",
        "print(f\"Sample rate: {sample_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccy6zfQ21VJo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_spectrogram(stft, title=\"Spectrogram\"):\n",
        "    magnitude = stft.abs()\n",
        "    spectrogram = 20 * torch.log10(magnitude + 1e-8).numpy()\n",
        "    _, axis = plt.subplots(1, 1)\n",
        "    axis.imshow(spectrogram, cmap=\"viridis\", vmin=-60, vmax=0, origin=\"lower\", aspect=\"auto\")\n",
        "    axis.set_title(title)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwvTLmyq4WCq"
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(wav_path)  # replace SAMPLE_SONG with desired path for different song\n",
        "waveform = waveform.to(device)\n",
        "mixture = waveform\n",
        "\n",
        "# parameters\n",
        "segment: int = 10\n",
        "overlap = 0.1\n",
        "\n",
        "print(\"Separating track\")\n",
        "\n",
        "ref = waveform.mean(0)\n",
        "waveform = (waveform - ref.mean()) / ref.std()  # normalization\n",
        "\n",
        "sources = separate_sources(\n",
        "    model,\n",
        "    waveform[None],\n",
        "    device=device,\n",
        "    segment=segment,\n",
        "    overlap=overlap,\n",
        ")[0]\n",
        "sources = sources * ref.std() + ref.mean()\n",
        "\n",
        "sources_list = model.sources\n",
        "sources = list(sources)\n",
        "\n",
        "audios = dict(zip(sources_list, sources))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5UHB_807mEe"
      },
      "outputs": [],
      "source": [
        "N_FFT = 4096\n",
        "N_HOP = 4\n",
        "stft = torchaudio.transforms.Spectrogram(\n",
        "    n_fft=N_FFT,\n",
        "    hop_length=N_HOP,\n",
        "    power=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTj3XGvI8S9D"
      },
      "outputs": [],
      "source": [
        "\n",
        "segment_start = 10\n",
        "segment_end = 15\n",
        "\n",
        "frame_start = segment_start * sample_rate\n",
        "frame_end = segment_end * sample_rate\n",
        "\n",
        "drums_spec = audios[\"drums\"][:, frame_start:frame_end].cpu()\n",
        "\n",
        "bass_spec = audios[\"bass\"][:, frame_start:frame_end].cpu()\n",
        "\n",
        "vocals_spec = audios[\"vocals\"][:, frame_start:frame_end].cpu()\n",
        "\n",
        "other_spec = audios[\"other\"][:, frame_start:frame_end].cpu()\n",
        "\n",
        "mix_spec = mixture[:, frame_start:frame_end].cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE6sLX-N8ZW2"
      },
      "outputs": [],
      "source": [
        "# Mixture Clip\n",
        "plot_spectrogram(stft(mix_spec)[0], \"Spectrogram - Mixture\")\n",
        "Audio(mix_spec, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2p2ylKu8gmO"
      },
      "outputs": [],
      "source": [
        "def output_results(predicted_source: torch.Tensor, source: str):\n",
        "    plot_spectrogram(stft(predicted_source)[0], f\"Spectrogram - {source}\")\n",
        "    return Audio(predicted_source, rate=sample_rate)\n",
        "\n",
        "\n",
        "output_results(vocals_spec, \"vocals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3_7CTWr9azm"
      },
      "outputs": [],
      "source": [
        "%pip install anki"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/japanese\n",
        "!rm /content/japanese"
      ],
      "metadata": {
        "id": "ewMvmNrACGJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anki\n",
        "from anki.collection import ImportAnkiPackageOptions, ImportAnkiPackageRequest\n",
        "import os\n",
        "\n",
        "\n",
        "col_name = \"japanese\"\n",
        "if os.path.exists(col_name):\n",
        "    os.remove(col_name)\n",
        "    #os.remove(f\"{col_name}-wal\")\n",
        "\n",
        "col = anki.collection.Collection(col_name)\n",
        "os.remove(col_name)\n",
        "os.remove(f\"{col_name}-wal\")\n",
        "\n",
        "\n",
        "col.import_anki_package(\n",
        "    ImportAnkiPackageRequest(\n",
        "        package_path=\"./Japanese__1.Kanji Study Words.apkg\",\n",
        "        options=ImportAnkiPackageOptions(\n",
        "            with_scheduling=True, with_deck_configs=True\n",
        "        ),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R25K8Idxofds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: deck id changes on loads\n",
        "# Get deck_id\n",
        "col.decks.all_names_and_ids()"
      ],
      "metadata": {
        "id": "kJbmLoVxw7r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deck_id = 1730494531756"
      ],
      "metadata": {
        "id": "NvHED6_aypqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get node ids\n",
        "card_ids = col.decks.cids(deck_id)\n",
        "note_types = set()\n",
        "for card_id in card_ids:\n",
        "    note_type = col.get_card(card_id).note_type()\n",
        "    note_types |= set([(note_type[\"id\"], note_type[\"name\"])])\n",
        "\n",
        "\n",
        "list(note_types)"
      ],
      "metadata": {
        "id": "5vzlvlrGEs80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a note type\n",
        "note_type_id = 1713569808917\n",
        "\n",
        "note_model = col.models.get(note_type_id)\n",
        "\n",
        "pitch = col.models.new_field(\"Pitch\")\n",
        "\n",
        "pitch[\"ord\"] = len(note_model['flds'])\n",
        "col.models.add_field(note_model, pitch)"
      ],
      "metadata": {
        "id": "h5_DEz2M0nKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note_ids = []\n",
        "deck_card_ids = col.decks.cids(deck_id)\n",
        "for cid in deck_card_ids:\n",
        "    c = col.get_card(cid)\n",
        "    if c.note_type()['id'] == note_type_id and c.nid not in note_ids:\n",
        "        note_ids.append(c.nid)\n",
        "\n",
        "\n",
        "choices = [nt['name'] for nt in col.models.get(note_type_id)['flds']]\n",
        "\n",
        "choices"
      ],
      "metadata": {
        "id": "acHg-uxo2KqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col.models.update_dict(note_model)"
      ],
      "metadata": {
        "id": "MKDjbnYy4eir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expr_fld = \"Sentence\"\n",
        "reading_fld = \"SentenceFurigana\"\n",
        "output_fld = \"Pitch\""
      ],
      "metadata": {
        "id": "k5km0Qd79x0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google_colab_selenium as gs\n",
        "\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "custom_options = Options()\n",
        "# Add your custom options here\n",
        "custom_options.add_argument('--lang=ja-JP')  # 日本語に設定\n",
        "custom_options.add_argument(\"--enable-javascript\")\n",
        "\n",
        "driver = gs.Chrome(options=custom_options)"
      ],
      "metadata": {
        "id": "iYUjlM2c_AVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "not_found_list = []\n",
        "num_updated = 0\n",
        "num_already_done = 0\n",
        "num_svg_fail = 0\n",
        "\n",
        "media_files = []\n",
        "\n",
        "for nid in note_ids:\n",
        "    # set up note access\n",
        "    note = col.get_note(nid)\n",
        "\n",
        "    filename = f\"{nid}_pitch\"\n",
        "\n",
        "    if len(note.fields) == note._field_index(output_fld):\n",
        "        note.fields.append('')\n",
        "\n",
        "\n",
        "    # check for existing illustrations\n",
        "    has_auto_accent = '<!-- accent_start -->' in note[output_fld]\n",
        "    has_manual_accent = '<!-- user_accent_start -->' in note[output_fld]\n",
        "    if has_auto_accent or has_manual_accent:\n",
        "        # already has a pitch accent illustration\n",
        "        num_already_done += 1\n",
        "        media_files.append(f'{filename}.png')\n",
        "        continue\n",
        "    # determine accent pattern\n",
        "    expr = note[expr_fld].strip()\n",
        "    reading = note[reading_fld].strip()\n",
        "    # remove brackets from furigana anotations and remove spaces\n",
        "    reading = \"\".join(re.split(\"\\[|\\]\", reading)[::2])\n",
        "\n",
        "    # generate png on OJAD website\n",
        "\n",
        "    img = get_pitched_text(reading, filename = filename)\n",
        "    if not img:\n",
        "        num_svg_fail += 1\n",
        "        continue\n",
        "    if len(note[output_fld]) > 0:\n",
        "        separator = '<br><hr><br>'\n",
        "    else:\n",
        "        separator = ''\n",
        "\n",
        "    media_files.append(f'{filename}.png')\n",
        "\n",
        "    # extend and save note\n",
        "    note[output_fld] = (\n",
        "        '{}<!-- accent_start -->{}{}<!-- accent_end -->'\n",
        "        ).format(note[output_fld], separator, f\"<img src='{img}'/>\")  # add img\n",
        "    col.update_note(note)\n",
        "    num_updated += 1\n",
        "\n",
        "\n",
        "media_folder = col.media.dir()\n",
        "\n",
        "print(media_folder)\n"
      ],
      "metadata": {
        "id": "IiULFYnn5Q4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "media_files = []\n",
        "\n",
        "for nid in note_ids:\n",
        "    filename = f\"{nid}_pitch\"\n",
        "    media_files.append(f'{filename}.png')"
      ],
      "metadata": {
        "id": "YYr5QnAX4mBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media_files"
      ],
      "metadata": {
        "id": "JfViFnme4fk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in media_files:\n",
        "    shutil.copyfile(f\"./{file}\", f\"{media_folder}/{file}\")"
      ],
      "metadata": {
        "id": "wR2PKbCpoEdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col.decks.get(deck_id)[\"name\"]"
      ],
      "metadata": {
        "id": "T6lbCWXQiefj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col.decks.rename(deck_id, 'Japanese::___Kanji Study Words')"
      ],
      "metadata": {
        "id": "waSBV7LpiWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hola\")"
      ],
      "metadata": {
        "id": "ZYIQ-vFe3_et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from anki.collection import DeckIdLimit, NoteIdsLimit\n",
        "import os\n",
        "\n",
        "#os.makedirs(\"test\")\n",
        "\n",
        "export_options = ExportAnkiPackageOptions(\n",
        "    with_scheduling=True, with_deck_configs=True, with_media=True\n",
        ")\n",
        "\n",
        "# Deck Limit only seems to work with a single deck,\n",
        "# So you could put the decks that you want to export into a subdeck of a bigger one\n",
        "# Or, concatenate the list of notes ids for the few decks that you want to update\n",
        "# to avoid deck layout changes\n",
        "export_limit = DeckIdLimit(deck_id=deck_id)\n",
        "\n",
        "\n",
        "export_limit = NoteIdsLimit(note_ids=note_ids)\n",
        "\n",
        "col.export_anki_package(\n",
        "    out_path=\"test/japanese_pitch4.apkg\",\n",
        "    options=export_options,\n",
        "    limit=export_limit\n",
        ")"
      ],
      "metadata": {
        "id": "iEEAyEsEj-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XlcoXO2aw4RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP4Vw2tdPivQZ2wIGJspDwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}